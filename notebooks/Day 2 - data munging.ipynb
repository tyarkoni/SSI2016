{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Corrections and clarifications from yesterday\n",
    "* Instructions for installing git\n",
    "    * Make sure you install [git](https://git-scm.com/downloads), not the GitHub client\n",
    "    * https://git-scm.com/downloads\n",
    "    * Once you've done that, open a terminal window (command prompt on Windows)\n",
    "    * type: `git clone https://github.com/tyarkoni/SSI2016.git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* If you have trouble installing Rpy2, try:\n",
    "    * `conda install -c r rpy2=2.7.0`\n",
    "    * But note that we won't use it again in this course\n",
    "* The Jupyter slideshow extension must be installed separately\n",
    "    * Follow instructions here: https://github.com/damianavila/RISE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview of Day 2\n",
    "* Motivation\n",
    "* A working dataset\n",
    "* Importing data\n",
    "* Pandas data structures\n",
    "* Preprocessing data\n",
    "* Working with text*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation\n",
    "* Data preparation is the un-sexy part of data science\n",
    "* But in many ways, the most important part\n",
    "* Choice of statistical model often makes no practical difference\n",
    "    * \"More data beats better algorithms\"\n",
    "* Small differences in preprocessing choices can ramify quickly\n",
    "    * E.g., variable smoothing or transformation, outlier removal, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dataset\n",
    "* The City of Austin has an [Open Data Portal](https://data.austintexas.gov/) containing many interesting datasets\n",
    "* We'll use [outcome data](https://data.austintexas.gov/Health/Austin-Animal-Center-Outcomes/jpst-ix7f) from the Austin Animal Center\n",
    "* 43,870 outcomes between October 2013, and February 2016\n",
    "* Variables include the type, age, sex, breed, name, and color of the animal, plus the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Some questions we can ask\n",
    "* What do people in Austin name their cats and dogs?\n",
    "* What are the most common dog and cat breeds processed at the animal center?\n",
    "* Do outcomes differ (e.g., euthanasia vs. adoption) for different breeds?\n",
    "    * Are purebred dogs more likely to be adopted than mixes?\n",
    "* Does animal size (e.g., bigger vs. smaller dogs) have any impact on outcomes?\n",
    "* How well can we predict the likely outcome given everything we know about an animal?\n",
    "* Do outcomes vary over time--e.g., by day of week, season, year, etc.?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Import all the things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# We'll consolidate all our imports at the top today\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# You may need to install the packages below. You can:\n",
    "# conda install beautifulsoup4 requests seaborn\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import seaborn as sns\n",
    "\n",
    "# Disable annoying SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Importing data\n",
    "* Before we do anything else, we need to get our data into a usable form\n",
    "* Most commonly, data will come from a flat file\n",
    "* But sometimes we need to retrieve data from other sources\n",
    "* We'll do both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reading data in with the standard library\n",
    "There are many ways to read in data in Python using the standard library. Here's a simple example, where we read in the data line-by-line and split each line into its own list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "filename = '../data/Austin_Animal_Center_Outcomes.csv'\n",
    "data = []  # Initialize an empty list to store the data\n",
    "\n",
    "# Loop over rows in the file, split each one into a list\n",
    "# of values, and add the result to the data list.\n",
    "for line in open(filename).readlines():\n",
    "    line = line.strip().split(',')\n",
    "    data.append(line)\n",
    "\n",
    "print(\"Found {} rows.\".format(len(data)))\n",
    "\n",
    "# Print the 1000th row to see what it looks like\n",
    "data[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The problem with approaches like the one above is that the data lack a tabular format, making it very hard to operate over rows or columns. We're much better off using the _pandas_ package to hold our data in a pandas DataFrame (DF)--a data structure that wraps around numpy arrays and is expressly designed to support a range of powerful operations over data. Reading a dataset into a pandas DF is very easy with the workhorse [read_csv()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) or [read_table()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html) methods. These methods take a large number of optional arguments that make it easy to read in almost any kind of orderly data represented in a text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Reading data, the pandas way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Note that we're reading the file directly from GitHub.\n",
    "# pandas accepts URLs in addition to local files.\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/tyarkoni/SSI2016/master/data/Austin_Animal_Center_Outcomes.csv'\n",
    "# If you're working from the cloned course GitHub repo, comment the line above and uncomment\n",
    "# the line below for faster loading.\n",
    "# url = '../data/Austin_Animal_Center_Outcomes.csv'\n",
    "\n",
    "# The workhorse data-reading method in pandas.\n",
    "# It accepts a LOT of optional arguments--\n",
    "# see http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# calling head() on a DataFrame shows the top N rows.\n",
    "data.head(5)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other formats\n",
    "Pandas has built-in support for [reading from or to other common formats/sources](http://pandas.pydata.org/pandas-docs/stable/io.html):\n",
    "* Generic delimited text -- read_table()\n",
    "* Excel -- read_excel()\n",
    "* JSON -- read_json()\n",
    "* SQL -- read_sql()\n",
    "* Stata -- read_stata()\n",
    "* SAS (XPORT or SAS7BDAT) -- read_sas()\n",
    "* etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scraping data\n",
    "* What if we want to add some data to our dataset?\n",
    "* It would be nice if we had height and weight estimates for dog breeds\n",
    "    * Are there different outcomes for bigger vs. smaller dogs?\n",
    "* We track down a website that has some [breed information](http://www.wisdompanel.com/breed_count_matters/breedlisting/?browseby=size&F_All=Y)\n",
    "* Now we need to \"scrape\" that data and get it into Python/pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "headers = { 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36'}\n",
    "url = \"http://www.wisdompanel.com/breed_count_matters/breedlisting/?browseby=size&F_All=Y\"\n",
    "\n",
    "html = requests.get(url, headers=headers).text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Extract links to all breeds with BeautifulSoup\n",
    "breed_links = soup.select('h3.itemTitle a')\n",
    "breed_links = [\"http://www.wisdompanel.com/\" + a['href'] for a in breed_links]\n",
    "print(\"Retrieving data for {} breeds.\".format(len(breed_links)))\n",
    "\n",
    "breed_data = []\n",
    "for breed in breed_links:\n",
    "\n",
    "    try:\n",
    "        breed_html = requests.get(breed, headers=headers).text\n",
    "\n",
    "        # Use regular expressions to extract name, height, width, description, and group\n",
    "        name = re.search('<h3>AKC Name:</h3>(.*?)<', breed_html, re.DOTALL).group(1)\n",
    "        min_weight, max_weight = re.search('<h3>Observed Weight:</h3>(\\d+)[\\s-]+(\\d+)', breed_html, re.DOTALL).groups()\n",
    "        height = re.search('Average Height:\\s+(\\d+)', breed_html, re.DOTALL).group(1)\n",
    "        group = re.search('<h3>Genetic Group:</h3>(.*?)<', breed_html, re.DOTALL).group(1)\n",
    "        breed_row = [name, min_weight, max_weight, height, group]\n",
    "        print(\"Retrieved breed data: \", breed_row)\n",
    "        breed_data.append(breed_row)\n",
    "        \n",
    "    # Some breed pages have formatting errors, so we'll just skip them\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Put the data in a pandas DataFrame\n",
    "breed_data = pd.DataFrame(breed_data, columns=['breed_name', 'min_weight', 'max_weight', 'height', 'group'])\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = os.path.join('..', 'data')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save the breed data locally for re-use\n",
    "output_file = os.path.join(output_dir, 'breed_data.csv')\n",
    "breed_data.to_csv(output_file, index=False, encoding='utf8')\n",
    "print(\"Successfully saved breed data to {}.\".format(output_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### The scraped breed data\n",
    "We'll come back later to the breed data we just scraped and saved. But for now, let's see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "breed_data = pd.read_csv('https://raw.githubusercontent.com/tyarkoni/SSI2016/master/data/breed_data.csv')\n",
    "\n",
    "breed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regular expressions\n",
    "* The heavy lifting in the previous scraping code is done with regular expressions\n",
    "* A powerful system for detecting and capturing patterns in text\n",
    "* One of the most underutilized features of programming languages\n",
    "* A short regular expression can replace dozens of lines of string-processing code\n",
    "* Lots of good tutorials ([1](https://developers.google.com/edu/python/regular-expressions), [2](http://regexone.com/references/python), [3](http://www.learnpython.org/en/Regular_Expressions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# A snippet of HTML from the Alaskan Malamute's breed\n",
    "# website on wisdompanel.com. We want to extract just the\n",
    "# genetic group information.\n",
    "text = '''\n",
    "<div><h3>AKC Name:</h3>Alaskan Malamute<br /><br /><h3>Common Name(s):</h3>Malamute<br /><br />\n",
    "<h3>AKC Group:</h3>Working<br /><br /><h3>Genetic Group:</h3>Asian<br /><br />\n",
    "<h3>Observed Weight:</h3>61-105 lbs<br /><br /><h3>Show Weight:</h3>68-92 lbs</div>\n",
    "'''\n",
    "\n",
    "# Extract with regular expressions. In this case,\n",
    "# We're looking to capture the group of all characters\n",
    "# that occur after '<h3>Genetic Group:</h3>'\n",
    "# and before '<'. We use \"lazy\" matching (the ? in\n",
    "# the group .*?) to indicate that we want the regex\n",
    "# engine to capture the _minimum_ amount of text possible\n",
    "# rather then default greedy behavior, which captures\n",
    "# as much text as possible.\n",
    "\n",
    "pattern = '<h3>Genetic Group:</h3>(.*?)<'\n",
    "\n",
    "# Search for pattern within text.\n",
    "# The re.DOTALL flag indicates that we want to match\n",
    "# across multiple lines of text, including newlines.\n",
    "matches = re.search(pattern, text, re.DOTALL)\n",
    "\n",
    "# Find the first match--i.e., the first group in our\n",
    "# pattern enclosed in parentheses.\n",
    "first_group = matches.group(1)\n",
    "\n",
    "print(first_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Some very simple regex examples\n",
    "\n",
    "text = \"A few additional examples (maybe 4 or 5) illustrating the power of regular expressions!!\"\n",
    "\n",
    "# All text beginning with 'add' and ending in 'power'\n",
    "print(re.search('(add.*power)',  text).group(1))\n",
    "\n",
    "# All numbers in the text\n",
    "print(re.findall('(\\d+)', text))\n",
    "\n",
    "# All words between 5 and 8 characters in length\n",
    "print(re.findall(r'\\b(\\w{5,8})\\b', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pandas data structures\n",
    "* Provides functionality similar to data frames in R\n",
    "* Two main data structures: Series and DataFrames\n",
    "* A Series is a 1-dimensional numpy array with axis labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a Series from a numpy array and index labels\n",
    "a = np.arange(3, 8)\n",
    "b = pd.Series(a, index=['apple', 'banana', 'orange', 'pear', 'grapes'])\n",
    "\n",
    "# Let's take a look...\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Unlike numpy arrays, we can now refer to elements by label.\n",
    "# The syntax is similar to dictionary indexing. You can also\n",
    "# treat labels like attributes (e.g., b.pear), but this runs\n",
    "# the risk of collisions and should be avoided.\n",
    "print(b['pear'])\n",
    "\n",
    "# We can always retrieve the underlying numpy array with .values\n",
    "print(b.values)\n",
    "\n",
    "# Many numpy operations work as expected, including slicing\n",
    "print(b[2:4])\n",
    "\n",
    "# Each column in our loaded dataset is a Series\n",
    "print(data['Breed'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The pandas DataFrame\n",
    "* The workhorse of data analysis in pandas\n",
    "* A container of multiple aligned Series\n",
    "* Heterogeneous: a DF's Series can have different dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Indexing pandas DataFrames\n",
    "* pandas DFs spport [flexible indexing](http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing) by labels and/or indices\n",
    "    * A common gotcha: R-style indexing won't work\n",
    "    * Be explicit about whether you're using integer or label indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# This won't work!\n",
    "data[0, 'Animal Type']\n",
    "\n",
    "# but .ix supports mixed integer and label based access\n",
    "data.ix[0, 'Animal Type']\n",
    "\n",
    "# Returns the entire column\n",
    "data['Animal Type']\n",
    "\n",
    "# Position-based selection; returns all of rows 2 - 5\n",
    "data.iloc[2:5]\n",
    "\n",
    "# Returns rows 2 - 5, columns 2 and 7\n",
    "data.iloc[2:5, [2, 7]]\n",
    "\n",
    "# Label-based indexing; equivalent to data['Animal Type']\n",
    "# in this case\n",
    "data.loc[:, 'Animal Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A quick summary of the data\n",
    "We'll cover descriptive analyses in more detail tomorrow, but for now, a basic summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preprocessing our data\n",
    "Much of what data scientists do involves cleaning and preprocessing data:\n",
    "* Handling missing or invalid values\n",
    "* Extracting usable information from messy strings\n",
    "* Transforming/normalizing variables and variable names\n",
    "* Filtering redundant or bad data\n",
    "* Merging with other datasets\n",
    "* Etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some examples\n",
    "* There's far too much to cover in 2 hours, so we'll cherry-pick useful examples\n",
    "* There are many excellent tutorials and guides (e.g., [1](http://www.gregreda.com/2013/10/26/intro-to-pandas-data-structures/), [2](http://synesthesiam.com/posts/an-introduction-to-pandas.html), [3](https://github.com/fonnesbeck/statistical-analysis-python-tutorial))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Renaming/replacing\n",
    "The column names in our dataset are kind of clunky. Let's make it a bit easier to work with the dataset by standardizing and shortening them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# This is no good\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Dictionary of old names --> new names.\n",
    "# Notice that we don't need to provide replacements\n",
    "# for _all_ the columns,  just the ones we want to\n",
    "# replace. An alternative approach would be to\n",
    "# directly set data.columns to a list of the\n",
    "# desired new column names.\n",
    "replacements = {\n",
    "    'Animal ID': 'id',\n",
    "    'DateTime': 'datetime',\n",
    "    'Outcome Type': 'outcome',\n",
    "    'Outcome Subtype': 'outcome_subtype',\n",
    "    'Animal Type': 'animal',\n",
    "    # We append these with '_string' because we're going\n",
    "    # to use the names 'sex' and 'age' later for\n",
    "    # extracted variables, so this avoids confusion.\n",
    "    'Sex upon Outcome': 'sex_string',\n",
    "    'Age upon Outcome': 'age_string',\n",
    "}\n",
    "\n",
    "# We can apply the rename function to either row names\n",
    "# or column names. Here we're doing columns.\n",
    "data = data.rename(columns=replacements)\n",
    "\n",
    "# Let's also make sure all column names are in lowercase.\n",
    "# We're using an idiom called a \"list comprehension\".\n",
    "data.columns = [c.lower() for c in data.columns]\n",
    "\n",
    "# The above list comprehension is functionally equivalent\n",
    "# to the following for-loop:\n",
    "# columns = []\n",
    "# for c in data.columns:\n",
    "#     columns.append(c)\n",
    "# data.columns = columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Working with time series data\n",
    "* The datetime column in our dataset is currently represented as a string\n",
    "* We want to treat it as a datetime, so we can easily extract information\n",
    "* Let's convert its time to datetime and pull out some useful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# These are actually strings/objects, not datetimes!\n",
    "print(data['datetime'][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the datetime column to actual datetimes\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "\n",
    "# Now we're working with datetimes\n",
    "print(data['datetime'][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we can easily extract some useful time-related variables for later use (e.g., year, day of week, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Datetime-related functions are accessed through the .dt attribute\n",
    "data['year'] = data['datetime'].dt.year\n",
    "data['hour'] = data['datetime'].dt.hour\n",
    "data['day'] = data['datetime'].dt.weekday\n",
    "\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Parsing text data\n",
    "* Many datasets contain nominal/categorical variables that need to be recoded as numeric variables\n",
    "* Three common strategies:\n",
    "    * Transform a nominal variable into a continuous one\n",
    "    * Break down a compound nominal label into constituent numeric variables\n",
    "    * \"Dummy-code\" a nominal variable as a set of binary variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Representing animal age with numeric values\n",
    "The age column in our DF has nasty values like \"3 years\" and \"7 months\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['age_string'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How do we convert these strings to numeric values we can operate on?\n",
    "\n",
    "We can take advantage of pandas' apply() functionality, which applies arbitrary methods to each value in a Series (or each column or row in a DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def extract_months(value):\n",
    "    ''' Extract and return age in months from string values \n",
    "    \"3 years\" or \"7 months\".\n",
    "    \n",
    "    Note: this code won't work as written, because some of\n",
    "    the values passed in are invalid.\n",
    "    \n",
    "    Exercise: Modify the code to return the special value\n",
    "    np.nan (numpy's representation of not-a-number)\n",
    "    whenever an invalid value is encountered.\n",
    "    '''\n",
    "    number, unit = value.split(' ')\n",
    "    number = int(number)\n",
    "\n",
    "    if unit.startswith('year'):\n",
    "        number *= 12\n",
    "\n",
    "    return number\n",
    "\n",
    "data['age'] = data['age_string'].apply(extract_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Notice any problems in the output? (Hint: week)\n",
    "# As an exercise, you can fix this by tweaking the\n",
    "# extract_months() method we wrote above and rerunning\n",
    "# the previous code block. \n",
    "data[['age_string', 'age']][:10] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Just for kicks...\n",
    "We're not ready to cover plotting just yet, but since we now have our first continuous variable, let's reward ourselves by plotting something nice to look at..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.violinplot(x='animal', y='age', data=data, hue='outcome')\n",
    "plt.gcf().set_size_inches((20, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Extracting data from the sex column\n",
    "* The original column combines sex and sterilization status into one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data['sex_string'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's recode this as two binary variables: sex (male/female) and sterilized (True/False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: The code below will fail the first time you run it. Why?\n",
    "# Because there's a single NaN value that messes up the indexing.\n",
    "# We need to recode the NaN--which we can do by setting it to\n",
    "# 'Unknown', just like the other 3,625 Unknown values. To do\n",
    "# that, uncomment the next line and re-run.\n",
    "\n",
    "# data['sex_string'] = data['sex_string'].fillna('Unknown')\n",
    "\n",
    "# Initialize a new column with NaN as the default value (for unknown sex)\n",
    "data['sex'] = np.nan\n",
    "\n",
    "# Update the column with values for M (0) and F (1)\n",
    "data['sex'][data['sex_string'].str.contains('Male')] = 0\n",
    "data['sex'][data['sex_string'].str.contains('Female')] = 1\n",
    "\n",
    "# Now do the same kind of thing for sterilization\n",
    "data['sterilized'] = 1\n",
    "data['sterilized'][data['sex_string'].str.contains('Intact')] = 0\n",
    "data['sterilized'][data['sex_string'].str.contains('Unknown')] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data[['sex_string', 'sex', 'sterilized']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note: an alternative way to do the above would have been to use pandas' .replace() method. As an exercise, rewrite the code above to achieve the same goal using .replace(). (Hint: create a dict of values to find and replace.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dummy-coded variables\n",
    "* Categorical variables with N levels often need to be modeled as N binary indicators\n",
    "* E.g., day-of-week is coded 0 through 6, but should be modeled as 6 or 7 contrasts\n",
    "* pandas makes this easy with get_dummies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We can also specify drop_first=True (on pandas >= 0.18) in order to\n",
    "# use N - 1 columns instead of N (e.g., to reduce collinearity).\n",
    "# It's also often useful to pass a prefix argument, which is prepended\n",
    "# to all the level names.\n",
    "pd.get_dummies(data['outcome'])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Filtering data\n",
    "* It's very common to want to filter out certain rows/columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dropping uncommon outcomes\n",
    "* Some outcomes in the dataset are too rare to merit analyzing\n",
    "    * We already saw how extra levels clutter up our plots and understanding\n",
    "* Let's impose a minimum cut-off of 200 occurrences for outcomes to be kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data['outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### A naive approach\n",
    "How would we solve this problem in pure Python, without pandas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Approach 1: the most naive approach, without\n",
    "# taking advantage of anything pandas has to offer.\n",
    "# This is slower and involves much more code than\n",
    "# the pandas-based solutions, but it works just fine.\n",
    "# To illustrate the logic behind what we're doing,\n",
    "# we use only native Python here, and no fancy idioms.\n",
    "# The other approaches do similar things under the\n",
    "# hood, but take advantage of many optimizations and\n",
    "# functions written in C to greatly speed things up.\n",
    "\n",
    "# Get the raw outcome values as a list\n",
    "values = data['outcome'].tolist()\n",
    "\n",
    "# Initialize a dictionary to store running counts of\n",
    "# all the outcomes.\n",
    "counts = {}\n",
    "\n",
    "# Loop over values and increment the appropriate\n",
    "# outcome's counter.\n",
    "for v in values:\n",
    "    if v not in counts:\n",
    "        counts[v] = 0\n",
    "    counts[v] += 1\n",
    "\n",
    "# Select the outcomes we want to keep--only those\n",
    "# above 1000 occurrences.\n",
    "min_count = 200\n",
    "\n",
    "# Initialize a list to store only the valid outcomes\n",
    "valid_outcomes = []\n",
    "\n",
    "# Loop over the outcome-count pairs in the dictionary\n",
    "# and keep only those above the threshold.\n",
    "for outcome, count in counts.items():\n",
    "    if count >= min_count:\n",
    "        valid_outcomes.append(outcome)\n",
    "\n",
    "# Identify the valid rows in the pandas DataFrame\n",
    "rows_to_keep = []\n",
    "for i, row in data.iterrows():\n",
    "    if row['outcome'] in valid_outcomes:\n",
    "        rows_to_keep.append(row)\n",
    "\n",
    "# Turn this list of Series back into a DataFrame\n",
    "filtered_data = pd.DataFrame(rows_to_keep)\n",
    "\n",
    "# Count the number of rows we have left and print them out\n",
    "n_rows = filtered_data.shape[0]\n",
    "print(\"After filtering, we have {} rows in the dataset.\".format(n_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### A better way\n",
    "* The pure Python approach works, but it's slow and very verbose\n",
    "* We can do much better with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Approach 2: We do essentially the same thing, but now we\n",
    "# take advantage of various built-in pandas methods.\n",
    "# Notice how much shorter--and faster!--the solution is.\n",
    "\n",
    "# Count the number of occurrences of each outcome\n",
    "counts = data['outcome'].value_counts()\n",
    "\n",
    "# Keep only those above a threshold--say 1000\n",
    "valid_outcomes = counts[counts >= 200].index\n",
    "\n",
    "# Now select only dataset rows with those outcomes\n",
    "valid_rows = data['outcome'].isin(valid_outcomes)\n",
    "filtered_data = data[valid_rows]\n",
    "\n",
    "print(\"After filtering, we have {} rows in the dataset.\".format(filtered_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### An even better way\n",
    "* That was much better. But we can do this even _more_ efficiently...\n",
    "* We'll use the split-apply-combine pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### The [split-apply-combine](https://www.jstatsoft.org/article/view/v040i01/v40i01.pdf) pattern\n",
    "* A very common data processing strategy\n",
    "    * Split the dataset into groups\n",
    "    * Apply some operation(s) to each group\n",
    "    * (Optionally) combine back into one dataset\n",
    "* pandas provides powerful and fast tools for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# A simple example calculate the mean age of each animal species\n",
    "\n",
    "# Split the dataset on animal. This creates a pandas GroupBy object.\n",
    "groups = data.groupby('animal')\n",
    "\n",
    "# pandas provides many built-in operations on groups--e.g., mean, count, etc.\n",
    "# see http://pandas.pydata.org/pandas-docs/stable/groupby.html#groupby-object-attributes\n",
    "groups['age'].mean()\n",
    "\n",
    "# The above line can be more explicitly written as:\n",
    "# groups['age'].aggregate('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We can apply arbitrary methods to each group.\n",
    "# Here we use a small \"lambda function\" (comparable to\n",
    "# an anonymous function in JavaScript, R, or Matlab)\n",
    "# to count the number of animals of each species\n",
    "# that are older than 12 months.\n",
    "groups['age'].apply(lambda x: (x > 12).sum())\n",
    "\n",
    "# A functionally equivalent, but more explicit way of\n",
    "# doing the above would be:\n",
    "\n",
    "# def count_animals_older_than_12_months(x):\n",
    "#     return (x > 12).sum()\n",
    "#\n",
    "# groups['age'].apply(count_animals_older_than_12_months)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### The one-line solution\n",
    "With the above pattern in mind, we can now turn back to our outcome-filtering problem and solve it in one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Approach 3: Use groupby() and a short lambda function.\n",
    "\n",
    "filtered_data = data.groupby('outcome').filter(lambda x: len(x) >= 200)\n",
    "\n",
    "print(\"After filtering, we have {} rows in the dataset.\".format(filtered_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Filtering animals for only the last outcome\n",
    "* Some animals have multiple outcomes in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Count number of outcomes per animal\n",
    "outcome_counts = data.groupby('id')['outcome'].count()\n",
    "print(outcome_counts[:5])\n",
    "\n",
    "# How many animals with more than one outcome?\n",
    "outcome_counts[outcome_counts > 1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We probably want to analyze only the _last_ outcome for each animal\n",
    "* This also becomes easy with the groupby strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Recall that we have a datetime column we can sort on,\n",
    "# to put all outcomes in chronological order.\n",
    "data = data.sort_values('datetime')\n",
    "\n",
    "# Group on animal and then use the built-in last() function\n",
    "filtered_data = data.groupby('id').last()\n",
    "\n",
    "# Note the handy comma injection the Python format method provides\n",
    "print(\"Before filtering: {:,} rows\".format(len(data)))\n",
    "print(\"After filtering: {:,} rows\".format(len(filtered_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Merging datasets\n",
    "* It's very common to need to combine data from different sources\n",
    "* Remember the breed data we scraped?\n",
    "* Let's merge it into our main DataFrame\n",
    "    * Will allow us to test whether genetic group, weight, or height matter\n",
    "* This is a more complicated operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The problem\n",
    "* We want to merge or 'join' our outcome data with the breed data\n",
    "* The obvious column to join on here is the breed name\n",
    "* But the names don't line up! Compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(data['breed'].value_counts().index[:10])\n",
    "print(breed_data['breed_name'].value_counts().index[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here's what we'll do:\n",
    "* Drop terms like \"mix\", \"shorthair\" and \"longhair\" that don't occur in the official names\n",
    "* For mixed breeds in our outcome data, we'll only use the first breed\n",
    "* For the most common breeds, we'll manually recode names so that they align\n",
    "* Note that all these steps are imperfect; our data will be noisy\n",
    "    * When munging data, we often need to decide how good is good enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Restrict analysis only to dogs, since that's all we have breed info for.\n",
    "# Notice our use of the query function, which enables us to perform SQL-like\n",
    "# queries over DataFrames.\n",
    "dogs = data.query('animal == \"Dog\"')\n",
    "\n",
    "# Let's prepare a 'merge_key' column in our outcome data that should line up\n",
    "# as well as possible with the breed_name column in the breed_data DataFrame.\n",
    "# We'll start by dropping all occurrences of 'Mix', 'Shorthair', etc.\n",
    "dogs['merge_key'] = dogs['breed'].replace([' Mix', ' Shorthair', ' Longhair'], '', regex=True)\n",
    "\n",
    "# Next, we'll keep only the first breed in cases where there are multiple given.\n",
    "# For example, \"Beagle/Labrador Retriever\" would become just \"Beagle\".\n",
    "dogs['merge_key'] = dogs['merge_key'].str.split('/').str.get(0)\n",
    "\n",
    "# Manually map the most common breeds that don't align. I've cheated here\n",
    "# and done a bit of extra work ahead of time to figure out what these are.\n",
    "replacements = {\n",
    "    \"Pit Bull\": \"American Staffordshire Terrier\",\n",
    "    \"Staffordshire\": \"Staffordshire Bull Terrier\",\n",
    "    \"German Shepherd\": \"German Shepherd Dog\",\n",
    "    \"Anatol Shepherd\": \"Anatolian Shepherd Dog\",\n",
    "    \"Australian Shepherd\": \"Australian Shepherd Dog\",\n",
    "    \"Catahoula\": \"Catahoula Leopard Dog\",\n",
    "    \"American Pit Bull Terrier\": \"American Staffordshire Terrier\", \n",
    "    \"Doberman Pinsch\": \"Doberman Pinscher\",  \n",
    "}\n",
    "\n",
    "# Apply the replacements\n",
    "dogs['merge_key'] = dogs['merge_key'].replace(replacements)\n",
    "\n",
    "# Finally, we're in a position to merge our data.\n",
    "# We have to tell pandas which columns to use in each of the two datasets.\n",
    "# We can also specify how we want the join to work ('left', 'outer',\n",
    "# 'inner', etc.). The default is left, meaning that the output DataFrame\n",
    "# will have the same number of rows as the first input DataFrame (dogs).\n",
    "dogs = dogs.merge(breed_data, left_on='merge_key', right_on='breed_name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dogs.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Your (optional) assignment for tomorrow\n",
    "* We extracted a numeric representation of several categorical variables\n",
    "    * We could extract many more\n",
    "* Some more variables we'll probably look at when we turn to fitting models:\n",
    "    * Animal color\n",
    "    * Whether the animal is a purebreed or mix\n",
    "    * Whether the outcome resulted in an animal's death (e.g., Died + Euthanasia)\n",
    "* Extract anything else you think might be useful"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
